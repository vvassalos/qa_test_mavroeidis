{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries\n!pip install datasets transformers --quiet\n!pip install pytorch-lightning --quiet\n\n# Regular imports\nimport numpy as np\nimport os\nimport pytorch_lightning as pl\nimport torch\nimport transformers\n\n# Specific imports\nfrom argparse import ArgumentParser\nfrom datasets import load_dataset, load_metric\nfrom itertools import compress\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-13T19:08:29.503553Z","iopub.execute_input":"2022-06-13T19:08:29.503972Z","iopub.status.idle":"2022-06-13T19:08:54.927068Z","shell.execute_reply.started":"2022-06-13T19:08:29.503942Z","shell.execute_reply":"2022-06-13T19:08:54.925469Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# Set up models\nmodel_checkpoint = 'distilbert-base-uncased'\ntransformer_model = transformers.AutoModel.from_pretrained(model_checkpoint)\ntransformer_tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\ntransformer_config = transformers.AutoConfig.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:08:54.930230Z","iopub.execute_input":"2022-06-13T19:08:54.930744Z","iopub.status.idle":"2022-06-13T19:09:02.278692Z","shell.execute_reply.started":"2022-06-13T19:08:54.930679Z","shell.execute_reply":"2022-06-13T19:09:02.277562Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"# Load the squad 1.1 dataset directly from transformers\ndatasets = load_dataset('squad')\nmetric = load_metric('squad')\nprint(datasets)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:09:02.284655Z","iopub.execute_input":"2022-06-13T19:09:02.287469Z","iopub.status.idle":"2022-06-13T19:09:04.426431Z","shell.execute_reply.started":"2022-06-13T19:09:02.287427Z","shell.execute_reply":"2022-06-13T19:09:04.424608Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data\nA script to collect and prepare the features which will be used for training. ","metadata":{}},{"cell_type":"code","source":"def prepare_train_features(examples):\n\n    # Maximum length of a feature (question and context)\n    max_length = 384 \n    # Authorized overlap between two part of the context \n    # when splitting it is needed.\n    doc_stride = 128 \n    tokenizer = transformer_tokenizer\n    # Padding side determines if we do (question|context) \n    # or (context|question).\n    pad_on_right = tokenizer.padding_side == 'right'\n    \n    # Tokenize our examples with truncation and padding, but keep the overflows\n    # using a stride. This results in one example possible giving several \n    # features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples['question' if pad_on_right else 'context'],\n        examples['context' if pad_on_right else 'question'],\n        truncation='only_second' if pad_on_right else 'only_first',\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding='max_length',\n    )\n\n    # Since one example might give us several features if it has a long context, \n    # we need a map from a feature to its corresponding example. \n    # This key gives us just that.\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    # The offset mappings will give us a map from token to character position \n    # in the original context. This will help us compute the start_positions \n    # and end_positions.\n    offset_mapping = tokenized_examples.pop('offset_mapping')\n\n    # Let's label those examples!\n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is \n        # the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the \n        # example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples['answers'][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers['answer_start']) == 0:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers['answer_start'][0]\n            end_char = start_char + len(answers['text'][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span \n            # (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and \n                    offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index \n                # to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the \n                # last word (edge case).\n                while token_start_index < len(offsets) and \\\n                        offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n\n    return tokenized_examples\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:09:04.430464Z","iopub.execute_input":"2022-06-13T19:09:04.430821Z","iopub.status.idle":"2022-06-13T19:09:04.447508Z","shell.execute_reply.started":"2022-06-13T19:09:04.430795Z","shell.execute_reply":"2022-06-13T19:09:04.446471Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"## Define the SQuAD data model","metadata":{}},{"cell_type":"code","source":"class SquadDataModule(pl.LightningDataModule):\n    def __init__(self, batch_size: int = 32, num_workers: int = 2):\n        super().__init__()\n   \n        # Defining batch size of our data\n        self.batch_size = batch_size\n          \n        # Defining num_workers\n        self.num_workers = num_workers\n\n        # Defining Tokenizers\n        self.tokenizer = transformer_tokenizer\n  \n    def prepare_data(self):\n        self.train_data = datasets['train']\n        self.val_data = datasets['validation']\n  \n    def setup(self, stage=None):\n        # Loading the dataset\n        self.train_dataset = self.train_data.map(\n            prepare_train_features, \n            batched=True, \n            remove_columns=self.train_data.column_names\n        )\n        self.val_dataset = self.val_data.map(\n            prepare_train_features, \n            batched=True, \n            remove_columns=self.val_data.column_names\n        )\n  \n    def custom_collate(self,features):\n        ## Pad the Batched data\n        batch = self.tokenizer.pad(  \n            features,\n            padding=True,\n            return_tensors='pt',\n        )\n        return batch\n        \n    def train_dataloader(self):\n        #dist_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n        #return DataLoader(train_dataset, sampler=dist_sampler, batch_size=32)\n        return DataLoader(\n            self.train_dataset, \n            shuffle=True, \n            batch_size=self.batch_size, \n            num_workers=self.num_workers, \n            collate_fn=self.custom_collate\n        )\n\n    def val_dataloader(self):\n         return DataLoader(\n             self.val_dataset,\n             batch_size=self.batch_size, \n             num_workers=self.num_workers, \n             collate_fn=self.custom_collate\n         )\n\n    # def test_dataloader(self):\n    #     return DataLoader(self.test_dataset, batch_size=self.batch_size, \n    #                       num_workers=self.num_workers, collate_fn=self.custom_collate)\n\n    def predict_dataloader(self):\n        return DataLoader(\n            self.val_dataset, \n            batch_size=self.batch_size, \n            num_workers=self.num_workers, \n            collate_fn=self.custom_collate\n        )","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:09:04.449141Z","iopub.execute_input":"2022-06-13T19:09:04.449993Z","iopub.status.idle":"2022-06-13T19:09:04.467495Z","shell.execute_reply.started":"2022-06-13T19:09:04.449947Z","shell.execute_reply":"2022-06-13T19:09:04.466463Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"## Define the SQuAD QA model","metadata":{}},{"cell_type":"code","source":"class SquadQAModel(pl.LightningModule):\n\n    def __init__(self, **kwargs):\n        super().__init__()\n        \n        self.transformer = transformer_model\n        # extract transformer name\n        transformer_name = self.transformer.name_or_path\n        # extract AutoConfig, from which relevant parameters can be extracted.\n        transformer_config = transformers.AutoConfig.from_pretrained(transformer_name)\n\n        self.num_labels = transformer_config.num_labels\n\n        self.qa_outputs = torch.nn.Linear(\n            transformer_config.hidden_size, \n            transformer_config.num_labels\n        )\n    \n    def forward(self, batch)-> torch.Tensor:\n        '''Model Forward Iteration\n        Args:\n            input_ids (torch.Tensor): Input IDs.\n            masks (torch.Tensor): Attention Masks.\n        Returns:\n            torch.Tensor: predicted values.\n        '''        \n\n        outputs = self.transformer(\n            input_ids = batch['input_ids'],\n            attention_mask=batch['attention_mask']\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return {'start_logits':start_logits, 'end_logits':end_logits }\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:09:04.471625Z","iopub.execute_input":"2022-06-13T19:09:04.472249Z","iopub.status.idle":"2022-06-13T19:09:04.485275Z","shell.execute_reply.started":"2022-06-13T19:09:04.472218Z","shell.execute_reply":"2022-06-13T19:09:04.484121Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"## Define the QA model","metadata":{}},{"cell_type":"code","source":"## The main Pytorch Lightning module\nclass SquadQuestionAnswering(pl.LightningModule):\n\n    def __init__(self, learning_rate: float = 2e-5, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Metrics   \n        self.model = SquadQAModel()         \n\n    def training_step(self, batch, batch_nb):\n        start_positions = batch['start_positions']\n        end_positions = batch['end_positions']\n\n        # fwd\n        output = self.model(batch)\n        start_logits = output['start_logits']\n        end_logits = output['end_logits']\n        \n        total_loss = self.compute_loss(\n            start_positions, \n            end_positions, \n            start_logits, \n            end_logits\n        )\n \n        return total_loss\n \n    def validation_step(self, batch, batch_nb):\n        start_positions = batch['start_positions']\n        end_positions = batch['end_positions']\n\n        # fwd\n        output = self.model(batch)\n        start_logits = output['start_logits']\n        end_logits = output['end_logits']\n        \n        # loss\n        total_loss = self.compute_loss(\n            start_positions, \n            end_positions, \n            start_logits, \n            end_logits\n        )\n        \n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log_dict({'val_loss':total_loss}, prog_bar=True)\n        return total_loss\n\n    def test_step(self, batch, batch_nb):\n        # loss - No test data in the dataset and that is why the loss is None\n        total_loss = None\n        self.log_dict({'test_loss':total_loss}, prog_bar=True)\n        return total_loss\n    \n    def predict_step(self, batch, batch_idx: int , dataloader_idx: int = None):\n        # fwd\n        output = self.model(batch)\n        start_logits = output['start_logits']\n        end_logits = output['end_logits']\n\n        return {'start_logits':start_logits, 'end_logits': end_logits}\n\n        return \n    def compute_loss(self, start_positions, end_positions, start_logits, end_logits):\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # Sometimes the start/end positions are outside our model inputs.\n            # We ignore these terms\n            ignored_index = start_logits.size(1)\n            # using clamp_(min, max) to make sure start_positions and \n            # end_positions don't go beyond max\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n        return total_loss \n\n    # ---------------------\n    # TRAINING SETUP\n    # ---------------------\n    def configure_optimizers(self):\n        '''\n        Configure the optimizer (Adam) and the scheduler to use\n        :return: The optimizer and the scheduler\n        '''\n        # Here, we could also use AdamW (Adam with weight decay), \n        # for improved generilization and speed of optimization\n        optimizer = torch.optim.Adam(\n            [p for p in self.parameters() if p.requires_grad], \n            lr=self.hparams.learning_rate, \n            eps=1e-08\n        )\n        scheduler = {\n        'scheduler': torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, \n            max_lr=2e-5, \n            steps_per_epoch=len(self.trainer.datamodule.train_dataloader()), \n            epochs=self.hparams.max_epochs\n        ),\n        'interval': 'step'  # called after each training step\n        } \n        #scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n        #scheduler = torch.optim.lr_scheduler.CyclicLR(\n        #    optimizer, \n        #    base_lr=1e-7, \n        #    max_lr=1e-4, \n        #    cycle_momentum=False,\n        #    step_size_up=300\n        #)\n        #scheduler = ReduceLROnPlateau(optimizer, patience=0, factor=0.2)\n        self.sched = scheduler\n        self.optim = optimizer\n        return [optimizer], [scheduler]\n \n    @staticmethod\n    def add_model_specific_args(parent_parser, root_dir):\n        '''\n        Define parameters that only apply to this model\n        '''\n        parser = ArgumentParser(parents=[parent_parser])\n\n        # network params\n        parser.add_argument('--drop_prob', default=0.2, type=float)\n\n        # data\n        parser.add_argument('--data_root', \n                            default=os.path.join(root_dir, 'train_val_data'), \n                            type=str)\n\n        # training params (opt)\n        parser.add_argument('--learning_rate', \n                            default=2e-5, \n                            type=float, \n                            help = 'type (default: %(default)f)')\n        return parser","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:09:04.487188Z","iopub.execute_input":"2022-06-13T19:09:04.488270Z","iopub.status.idle":"2022-06-13T19:09:04.514943Z","shell.execute_reply.started":"2022-06-13T19:09:04.488226Z","shell.execute_reply":"2022-06-13T19:09:04.513914Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"## Define training parameters","metadata":{}},{"cell_type":"code","source":"root_dir = os.getcwd()\nparent_parser = ArgumentParser(add_help=False)\nparent_parser = pl.Trainer.add_argparse_args(parent_parser)\n\n# each LightningModule defines arguments relevant to it\nparser = SquadQuestionAnswering.add_model_specific_args(parent_parser,root_dir)\n\nparser.set_defaults(\n    #profiler='simple',\n    deterministic=True,\n    max_epochs=3,\n    limit_train_batches=1.0,\n    limit_val_batches=1.0,\n    limit_test_batches=1.0,\n    gpus=1,\n    distributed_backend=None,\n    fast_dev_run=False,\n    model_load=False,\n    model_name='best_model',\n)\n\nargs, extra = parser.parse_known_args()\n\n''' Main training routine specific for this project. '''\n# ------------------------\n# 1 INIT LIGHTNING MODEL\n# ------------------------\nif (vars(args)['model_load']):\n  model = SquadQuestionAnswering.load_from_checkpoint(vars(args)['model_name'])\nelse:  \n  model = SquadQuestionAnswering(**vars(args))\n\n# ------------------------\n# 2 CALLBACKS of MODEL\n# ------------------------\n\n# callbacks\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0,\n    patience=3,\n    verbose=True,\n    mode='min',\n    strict=True,\n)\n\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\ncheckpoint_callback = ModelCheckpoint(\n     monitor='val_loss',\n     #dirpath='my/path/',\n     filename='squad-questionanswer-epoch{epoch:02d}-val_loss{val_loss:.2f}',\n     auto_insert_metric_name=False\n)\n\n# ------------------------\n# 3 INIT TRAINER\n# ------------------------\ntrainer = Trainer.from_argparse_args(args,\n    callbacks=[early_stop,lr_monitor, checkpoint_callback]\n    )    \n\nseed_everything(42, workers=True)\nsquad_dm = SquadDataModule()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:09:04.517126Z","iopub.execute_input":"2022-06-13T19:09:04.517647Z","iopub.status.idle":"2022-06-13T19:09:04.981897Z","shell.execute_reply.started":"2022-06-13T19:09:04.517603Z","shell.execute_reply":"2022-06-13T19:09:04.980935Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"trainer.fit(model,squad_dm)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:09:04.983182Z","iopub.execute_input":"2022-06-13T19:09:04.983630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare features for validation","metadata":{}},{"cell_type":"code","source":"def prepare_validation_features(examples):\n    # The maximum length of a feature (question and context)\n    max_length = 384 \n    # The authorized overlap between two part of the context when splitting it is needed.\n    doc_stride = 128 \n    tokenizer = transformer_tokenizer\n    # Padding side determines if we do (question|context) or (context|question).\n    pad_on_right = tokenizer.padding_side == 'right'\n\n    # Tokenize our examples with truncation and maybe padding, \n    # but keep the overflows using a stride. This results\n    # in one example possible giving several features when a \n    # context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples['question' if pad_on_right else 'context'],\n        examples['context' if pad_on_right else 'question'],\n        truncation='only_second' if pad_on_right else 'only_first',\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding='max_length',\n    )\n\n    # Since one example might give us several features if it has a \n    # long context, we need a map from a feature to its corresponding example. \n    # This key gives us just that.\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n\n    # We keep the example_id that gave us this feature and we will store the \n    # offset mappings.\n    tokenized_examples['example_id'] = []\n\n    for i in range(len(tokenized_examples['input_ids'])):\n        # Grab the sequence corresponding to that example \n        # (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n\n        # One example can give several spans, this is the index of \n        # the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples['id'][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context \n        # so it's easy to determine if a token position is part of \n        # the context or not.\n        tokenized_examples['offset_mapping'][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples['offset_mapping'][i])\n        ]\n\n    return tokenized_examples\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Define a smaller validation set to quickly assess performance\ndataset_valid = datasets['validation'].select([idx for idx in range(16)])\n# dataset_valid = datasets['validation']\nvalidation_features = dataset_valid.map(\n    prepare_validation_features,\n    batched=True,\n    remove_columns=datasets['validation'].column_names\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_collate(features):\n    tokenizer = transformer_tokenizer\n    ## Have to remove these to make the tensors\n    example_ids = [feature.pop('example_id') for feature in features]\n    offset_mapping =  [feature.pop('offset_mapping') for feature in features]\n    ## Pad the Batched data\n    batch = tokenizer.pad(  \n        features,\n        padding=True,\n        return_tensors='pt',\n    )\n    return batch\n\nval_dataloader = DataLoader(\n    validation_features,\n    batch_size=8, \n    num_workers=2, \n    collate_fn=custom_collate\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get predictions","metadata":{}},{"cell_type":"code","source":"raw_predictions = trainer.predict(model,dataloaders=val_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport collections\n\ndef postprocess_qa_predictions(\n        examples, \n        features, \n        raw_predictions, \n        n_best_size = 20, \n        max_answer_length = 30\n):\n    tokenizer = transformer_tokenizer\n    all_start_logits = torch.cat([predict['start_logits'] \\\n                                  for predict in raw_predictions]).cpu().numpy()\n    all_end_logits = torch.cat([predict['end_logits'] \\\n                                for predict in raw_predictions]).cpu().numpy()\n    example_id_to_index = {k: i for i, k in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f'Post-processing {len(examples)} example ' \\\n          'predictions split into {len(features)} features.')\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n        valid_answers = []\n        \n        context = example['context']\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions \n            # in our logits to span of texts in the original context.\n            offset_mapping = features[feature_index]['offset_mapping']\n\n            # Update minimum null prediction.\n            cls_index = features[feature_index]['input_ids'].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n\n            # Go through all possibilities for the `n_best_size` \n            # greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, \n                    # either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is \n                    # either < 0 or > max_answer_length.\n                    if (end_index < start_index or \n                        end_index - start_index + 1 > max_answer_length):\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            'score': start_logits[start_index] + end_logits[end_index],\n                            'text': context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x['score'], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, \n            # we create a fake prediction to avoid failure.\n            best_answer = {'text': '', 'score': 0.0}\n        \n        answer = best_answer['text'] # if best_answer['score'] > min_null_score else ''\n        predictions[example['id']] = answer\n\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = postprocess_qa_predictions(\n    dataset_valid, \n    validation_features, \n    raw_predictions\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions['56be4db0acb8001400a502ec']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric = load_metric('squad')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"formatted_predictions = [{'id': k, 'prediction_text': v} for k, v in final_predictions.items()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"references = [{'id': ex['id'], 'answers': ex['answers']} for ex in dataset_valid]\nmetric.compute(predictions=formatted_predictions, references=references)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Define a smaller validation set to quickly assess performance\ndataset_valid = datasets['validation'].select([idx for idx in range(16)])\nvalidation_features = dataset_valid.map(\n    prepare_validation_features,\n    batched=True,\n    remove_columns=datasets['validation'].column_names\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataloader = DataLoader(\n    validation_features,\n    batch_size=8, \n    num_workers=2, \n    collate_fn=custom_collate\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = SquadQuestionAnswering.load_from_checkpoint('lightning_logs/version_0/checkpoints/squad-questionanswer-epoch00-val_loss0.20.ckpt')\nmodel = SquadQuestionAnswering()\nraw_predictions = trainer.predict(model,dataloaders=val_dataloader)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = postprocess_qa_predictions(\n    dataset_valid, \n    validation_features, \n    raw_predictions\n)\nprint(final_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model to disk\ntorch.save(trainer, 'model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}