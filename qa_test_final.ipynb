{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-13T19:08:29.503972Z",
     "iopub.status.busy": "2022-06-13T19:08:29.503553Z",
     "iopub.status.idle": "2022-06-13T19:08:54.927068Z",
     "shell.execute_reply": "2022-06-13T19:08:54.925469Z",
     "shell.execute_reply.started": "2022-06-13T19:08:29.503942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install datasets transformers --quiet\n",
    "!pip install pytorch-lightning --quiet\n",
    "\n",
    "# Regular imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Specific imports\n",
    "from argparse import ArgumentParser\n",
    "from datasets import load_dataset, load_metric\n",
    "from itertools import compress\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T19:08:54.930744Z",
     "iopub.status.busy": "2022-06-13T19:08:54.930230Z",
     "iopub.status.idle": "2022-06-13T19:09:02.278692Z",
     "shell.execute_reply": "2022-06-13T19:09:02.277562Z",
     "shell.execute_reply.started": "2022-06-13T19:08:54.930679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up models\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "transformer_model = transformers.AutoModel.from_pretrained(model_checkpoint)\n",
    "transformer_tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "transformer_config = transformers.AutoConfig.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T19:09:02.287469Z",
     "iopub.status.busy": "2022-06-13T19:09:02.284655Z",
     "iopub.status.idle": "2022-06-13T19:09:04.426431Z",
     "shell.execute_reply": "2022-06-13T19:09:04.424608Z",
     "shell.execute_reply.started": "2022-06-13T19:09:02.287427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the squad 1.1 dataset directly from transformers\n",
    "datasets = load_dataset('squad')\n",
    "metric = load_metric('squad')\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "A script to collect and prepare the features which will be used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T19:09:04.430821Z",
     "iopub.status.busy": "2022-06-13T19:09:04.430464Z",
     "iopub.status.idle": "2022-06-13T19:09:04.447508Z",
     "shell.execute_reply": "2022-06-13T19:09:04.446471Z",
     "shell.execute_reply.started": "2022-06-13T19:09:04.430795Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "\n",
    "    # Maximum length of a feature (question and context)\n",
    "    max_length = 384 \n",
    "    # Authorized overlap between two part of the context \n",
    "    # when splitting it is needed.\n",
    "    doc_stride = 128 \n",
    "    tokenizer = transformer_tokenizer\n",
    "    # Padding side determines if we do (question|context) \n",
    "    # or (context|question).\n",
    "    pad_on_right = tokenizer.padding_side == 'right'\n",
    "    \n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows\n",
    "    # using a stride. This results in one example possible giving several \n",
    "    # features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['question' if pad_on_right else 'context'],\n",
    "        examples['context' if pad_on_right else 'question'],\n",
    "        truncation='only_second' if pad_on_right else 'only_first',\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, \n",
    "    # we need a map from a feature to its corresponding example. \n",
    "    # This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n",
    "    # The offset mappings will give us a map from token to character position \n",
    "    # in the original context. This will help us compute the start_positions \n",
    "    # and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop('offset_mapping')\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples['start_positions'] = []\n",
    "    tokenized_examples['end_positions'] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples['input_ids'][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is \n",
    "        # the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the \n",
    "        # example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples['answers'][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers['answer_start']) == 0:\n",
    "            tokenized_examples['start_positions'].append(cls_index)\n",
    "            tokenized_examples['end_positions'].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers['answer_start'][0]\n",
    "            end_char = start_char + len(answers['text'][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span \n",
    "            # (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and \n",
    "                    offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples['start_positions'].append(cls_index)\n",
    "                tokenized_examples['end_positions'].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index \n",
    "                # to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the \n",
    "                # last word (edge case).\n",
    "                while token_start_index < len(offsets) and \\\n",
    "                        offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples['start_positions'].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples['end_positions'].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the SQuAD data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T19:09:04.449993Z",
     "iopub.status.busy": "2022-06-13T19:09:04.449141Z",
     "iopub.status.idle": "2022-06-13T19:09:04.467495Z",
     "shell.execute_reply": "2022-06-13T19:09:04.466463Z",
     "shell.execute_reply.started": "2022-06-13T19:09:04.449947Z"
    }
   },
   "outputs": [],
   "source": [
    "class SquadDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 32, num_workers: int = 2):\n",
    "        super().__init__()\n",
    "   \n",
    "        # Defining batch size of our data\n",
    "        self.batch_size = batch_size\n",
    "          \n",
    "        # Defining num_workers\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Defining Tokenizers\n",
    "        self.tokenizer = transformer_tokenizer\n",
    "  \n",
    "    def prepare_data(self):\n",
    "        self.train_data = datasets['train']\n",
    "        self.val_data = datasets['validation']\n",
    "  \n",
    "    def setup(self, stage=None):\n",
    "        # Loading the dataset\n",
    "        self.train_dataset = self.train_data.map(\n",
    "            prepare_train_features, \n",
    "            batched=True, \n",
    "            remove_columns=self.train_data.column_names\n",
    "        )\n",
    "        self.val_dataset = self.val_data.map(\n",
    "            prepare_train_features, \n",
    "            batched=True, \n",
    "            remove_columns=self.val_data.column_names\n",
    "        )\n",
    "  \n",
    "    def custom_collate(self,features):\n",
    "        ## Pad the Batched data\n",
    "        batch = self.tokenizer.pad(  \n",
    "            features,\n",
    "            padding=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return batch\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        #dist_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "        #return DataLoader(train_dataset, sampler=dist_sampler, batch_size=32)\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            shuffle=True, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers, \n",
    "            collate_fn=self.custom_collate\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "         return DataLoader(\n",
    "             self.val_dataset,\n",
    "             batch_size=self.batch_size, \n",
    "             num_workers=self.num_workers, \n",
    "             collate_fn=self.custom_collate\n",
    "         )\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.test_dataset, batch_size=self.batch_size, \n",
    "    #                       num_workers=self.num_workers, collate_fn=self.custom_collate)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers, \n",
    "            collate_fn=self.custom_collate\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the SQuAD QA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T19:09:04.472249Z",
     "iopub.status.busy": "2022-06-13T19:09:04.471625Z",
     "iopub.status.idle": "2022-06-13T19:09:04.485275Z",
     "shell.execute_reply": "2022-06-13T19:09:04.484121Z",
     "shell.execute_reply.started": "2022-06-13T19:09:04.472218Z"
    }
   },
   "outputs": [],
   "source": [
    "class SquadQAModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = transformer_model\n",
    "        # extract transformer name\n",
    "        transformer_name = self.transformer.name_or_path\n",
    "        # extract AutoConfig, from which relevant parameters can be extracted.\n",
    "        transformer_config = transformers.AutoConfig.from_pretrained(transformer_name)\n",
    "\n",
    "        self.num_labels = transformer_config.num_labels\n",
    "\n",
    "        self.qa_outputs = torch.nn.Linear(\n",
    "            transformer_config.hidden_size, \n",
    "            transformer_config.num_labels\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch)-> torch.Tensor:\n",
    "        '''Model Forward Iteration\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input IDs.\n",
    "            masks (torch.Tensor): Attention Masks.\n",
    "        Returns:\n",
    "            torch.Tensor: predicted values.\n",
    "        '''        \n",
    "\n",
    "        outputs = self.transformer(\n",
    "            input_ids = batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return {'start_logits':start_logits, 'end_logits':end_logits }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the QA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T19:09:04.488270Z",
     "iopub.status.busy": "2022-06-13T19:09:04.487188Z",
     "iopub.status.idle": "2022-06-13T19:09:04.514943Z",
     "shell.execute_reply": "2022-06-13T19:09:04.513914Z",
     "shell.execute_reply.started": "2022-06-13T19:09:04.488226Z"
    }
   },
   "outputs": [],
   "source": [
    "## The main Pytorch Lightning module\n",
    "class SquadQuestionAnswering(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, learning_rate: float = 2e-5, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Metrics   \n",
    "        self.model = SquadQAModel()         \n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        start_positions = batch['start_positions']\n",
    "        end_positions = batch['end_positions']\n",
    "\n",
    "        # fwd\n",
    "        output = self.model(batch)\n",
    "        start_logits = output['start_logits']\n",
    "        end_logits = output['end_logits']\n",
    "        \n",
    "        total_loss = self.compute_loss(\n",
    "            start_positions, \n",
    "            end_positions, \n",
    "            start_logits, \n",
    "            end_logits\n",
    "        )\n",
    " \n",
    "        return total_loss\n",
    " \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        start_positions = batch['start_positions']\n",
    "        end_positions = batch['end_positions']\n",
    "\n",
    "        # fwd\n",
    "        output = self.model(batch)\n",
    "        start_logits = output['start_logits']\n",
    "        end_logits = output['end_logits']\n",
    "        \n",
    "        # loss\n",
    "        total_loss = self.compute_loss(\n",
    "            start_positions, \n",
    "            end_positions, \n",
    "            start_logits, \n",
    "            end_logits\n",
    "        )\n",
    "        \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log_dict({'val_loss':total_loss}, prog_bar=True)\n",
    "        return total_loss\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # loss - No test data in the dataset and that is why the loss is None\n",
    "        total_loss = None\n",
    "        self.log_dict({'test_loss':total_loss}, prog_bar=True)\n",
    "        return total_loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx: int , dataloader_idx: int = None):\n",
    "        # fwd\n",
    "        output = self.model(batch)\n",
    "        start_logits = output['start_logits']\n",
    "        end_logits = output['end_logits']\n",
    "\n",
    "        return {'start_logits':start_logits, 'end_logits': end_logits}\n",
    "\n",
    "        return \n",
    "    def compute_loss(self, start_positions, end_positions, start_logits, end_logits):\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # Sometimes the start/end positions are outside our model inputs.\n",
    "            # We ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            # using clamp_(min, max) to make sure start_positions and \n",
    "            # end_positions don't go beyond max\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "        return total_loss \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        Configure the optimizer (Adam) and the scheduler to use\n",
    "        :return: The optimizer and the scheduler\n",
    "        '''\n",
    "        # Here, we could also use AdamW (Adam with weight decay), \n",
    "        # for improved generilization and speed of optimization\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [p for p in self.parameters() if p.requires_grad], \n",
    "            lr=self.hparams.learning_rate, \n",
    "            eps=1e-08\n",
    "        )\n",
    "        scheduler = {\n",
    "        'scheduler': torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=2e-5, \n",
    "            steps_per_epoch=len(self.trainer.datamodule.train_dataloader()), \n",
    "            epochs=self.hparams.max_epochs\n",
    "        ),\n",
    "        'interval': 'step'  # called after each training step\n",
    "        } \n",
    "        #scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "        #scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "        #    optimizer, \n",
    "        #    base_lr=1e-7, \n",
    "        #    max_lr=1e-4, \n",
    "        #    cycle_momentum=False,\n",
    "        #    step_size_up=300\n",
    "        #)\n",
    "        #scheduler = ReduceLROnPlateau(optimizer, patience=0, factor=0.2)\n",
    "        self.sched = scheduler\n",
    "        self.optim = optimizer\n",
    "        return [optimizer], [scheduler]\n",
    " \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser, root_dir):\n",
    "        '''\n",
    "        Define parameters that only apply to this model\n",
    "        '''\n",
    "        parser = ArgumentParser(parents=[parent_parser])\n",
    "\n",
    "        # network params\n",
    "        parser.add_argument('--drop_prob', default=0.2, type=float)\n",
    "\n",
    "        # data\n",
    "        parser.add_argument('--data_root', \n",
    "                            default=os.path.join(root_dir, 'train_val_data'), \n",
    "                            type=str)\n",
    "\n",
    "        # training params (opt)\n",
    "        parser.add_argument('--learning_rate', \n",
    "                            default=2e-5, \n",
    "                            type=float, \n",
    "                            help = 'type (default: %(default)f)')\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T19:09:04.517647Z",
     "iopub.status.busy": "2022-06-13T19:09:04.517126Z",
     "iopub.status.idle": "2022-06-13T19:09:04.981897Z",
     "shell.execute_reply": "2022-06-13T19:09:04.980935Z",
     "shell.execute_reply.started": "2022-06-13T19:09:04.517603Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "parent_parser = ArgumentParser(add_help=False)\n",
    "parent_parser = pl.Trainer.add_argparse_args(parent_parser)\n",
    "\n",
    "# each LightningModule defines arguments relevant to it\n",
    "parser = SquadQuestionAnswering.add_model_specific_args(parent_parser,root_dir)\n",
    "\n",
    "parser.set_defaults(\n",
    "    #profiler='simple',\n",
    "    deterministic=True,\n",
    "    max_epochs=3,\n",
    "    limit_train_batches=1.0,\n",
    "    limit_val_batches=1.0,\n",
    "    limit_test_batches=1.0,\n",
    "    gpus=1,\n",
    "    distributed_backend=None,\n",
    "    fast_dev_run=False,\n",
    "    model_load=False,\n",
    "    model_name='best_model',\n",
    ")\n",
    "\n",
    "args, extra = parser.parse_known_args()\n",
    "\n",
    "''' Main training routine specific for this project. '''\n",
    "\n",
    "if (vars(args)['model_load']):\n",
    "  model = SquadQuestionAnswering.load_from_checkpoint(vars(args)['model_name'])\n",
    "else:  \n",
    "  model = SquadQuestionAnswering(**vars(args))\n",
    "\n",
    "# callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min',\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "     monitor='val_loss',\n",
    "     #dirpath='my/path/',\n",
    "     filename='squad-questionanswer-epoch{epoch:02d}-val_loss{val_loss:.2f}',\n",
    "     auto_insert_metric_name=False\n",
    ")\n",
    "\n",
    "trainer = Trainer.from_argparse_args(args,\n",
    "    callbacks=[early_stop,lr_monitor, checkpoint_callback]\n",
    ")    \n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "squad_dm = SquadDataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T19:09:04.983630Z",
     "iopub.status.busy": "2022-06-13T19:09:04.983182Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(model,squad_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare features for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # The maximum length of a feature (question and context)\n",
    "    max_length = 384 \n",
    "    # The authorized overlap between two part of the context when splitting it is needed.\n",
    "    doc_stride = 128 \n",
    "    tokenizer = transformer_tokenizer\n",
    "    # Padding side determines if we do (question|context) or (context|question).\n",
    "    pad_on_right = tokenizer.padding_side == 'right'\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, \n",
    "    # but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a \n",
    "    # context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['question' if pad_on_right else 'context'],\n",
    "        examples['context' if pad_on_right else 'question'],\n",
    "        truncation='only_second' if pad_on_right else 'only_first',\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a \n",
    "    # long context, we need a map from a feature to its corresponding example. \n",
    "    # This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the \n",
    "    # offset mappings.\n",
    "    tokenized_examples['example_id'] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples['input_ids'])):\n",
    "        # Grab the sequence corresponding to that example \n",
    "        # (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of \n",
    "        # the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples['example_id'].append(examples['id'][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context \n",
    "        # so it's easy to determine if a token position is part of \n",
    "        # the context or not.\n",
    "        tokenized_examples['offset_mapping'][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples['offset_mapping'][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller validation set to quickly assess performance\n",
    "dataset_valid = datasets['validation'].select([idx for idx in range(16)])\n",
    "# dataset_valid = datasets['validation']\n",
    "validation_features = dataset_valid.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets['validation'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(features):\n",
    "    tokenizer = transformer_tokenizer\n",
    "    ## Have to remove these to make the tensors\n",
    "    example_ids = [feature.pop('example_id') for feature in features]\n",
    "    offset_mapping =  [feature.pop('offset_mapping') for feature in features]\n",
    "    ## Pad the Batched data\n",
    "    batch = tokenizer.pad(  \n",
    "        features,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    return batch\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    validation_features,\n",
    "    batch_size=8, \n",
    "    num_workers=2, \n",
    "    collate_fn=custom_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = trainer.predict(model,dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "\n",
    "def postprocess_qa_predictions(\n",
    "        examples, \n",
    "        features, \n",
    "        raw_predictions, \n",
    "        n_best_size = 20, \n",
    "        max_answer_length = 30\n",
    "):\n",
    "    tokenizer = transformer_tokenizer\n",
    "    all_start_logits = torch.cat([predict['start_logits'] \\\n",
    "                                  for predict in raw_predictions]).cpu().numpy()\n",
    "    all_end_logits = torch.cat([predict['end_logits'] \\\n",
    "                                for predict in raw_predictions]).cpu().numpy()\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples['id'])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature['example_id']]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f'Post-processing {len(examples)} example ' \\\n",
    "          'predictions split into {len(features)} features.')\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example['context']\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions \n",
    "            # in our logits to span of texts in the original context.\n",
    "            offset_mapping = features[feature_index]['offset_mapping']\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index]['input_ids'].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` \n",
    "            # greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, \n",
    "                    # either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is \n",
    "                    # either < 0 or > max_answer_length.\n",
    "                    if (end_index < start_index or \n",
    "                        end_index - start_index + 1 > max_answer_length):\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            'score': start_logits[start_index] + end_logits[end_index],\n",
    "                            'text': context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x['score'], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, \n",
    "            # we create a fake prediction to avoid failure.\n",
    "            best_answer = {'text': '', 'score': 0.0}\n",
    "        \n",
    "        answer = best_answer['text'] # if best_answer['score'] > min_null_score else ''\n",
    "        predictions[example['id']] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = postprocess_qa_predictions(\n",
    "    dataset_valid, \n",
    "    validation_features, \n",
    "    raw_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_predictions = [{'id': k, 'prediction_text': v} for k, v in final_predictions.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [{'id': ex['id'], 'answers': ex['answers']} for ex in dataset_valid]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a smaller validation set to quickly assess performance\n",
    "dataset_valid = datasets['validation'].select([idx for idx in range(16)])\n",
    "validation_features = dataset_valid.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets['validation'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    validation_features,\n",
    "    batch_size=8, \n",
    "    num_workers=2, \n",
    "    collate_fn=custom_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SquadQuestionAnswering.load_from_checkpoint('lightning_logs/version_0/checkpoints/squad-questionanswer-epoch00-val_loss0.20.ckpt')\n",
    "model = SquadQuestionAnswering()\n",
    "raw_predictions = trainer.predict(model,dataloaders=val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = postprocess_qa_predictions(\n",
    "    dataset_valid, \n",
    "    validation_features, \n",
    "    raw_predictions\n",
    ")\n",
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk\n",
    "torch.save(trainer, 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
